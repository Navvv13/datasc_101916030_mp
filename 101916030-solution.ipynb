{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"df = pd.read_json('/kaggle/input/github-bugs-prediction/embold_train.json')\ndf['combined'] = df['title']+'. '+df['body']\ndf.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"df_bug = df[df['label']==0]\ndf_feature = df[df['label']==1]\ndf_question = df[df['label']==2]","metadata":{"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport re\nimport string\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n\n    return text","metadata":{"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"from string import punctuation\nfrom nltk.corpus import stopwords\n\ndef punctuation_stopwords_removal(git_text):\n    remove_punctuation = [ch for ch in git_text if ch not in punctuation]\n    remove_punctuation = \"\".join(remove_punctuation).split()\n    filtered_git_text = [word.lower() for word in remove_punctuation if word.lower() not in stopwords.words('english')]\n    return filtered_git_text","metadata":{"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nimport plotly.express as px\n\ndef plot_most_common_words(df_category, category):\n    df_category['combined'] = df_category['combined'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\n    df_category['combined'] = df_category['combined'].apply(lambda x: clean_text(x))\n    \n    df_category[\"combined\"] = df_category[\"combined\"].apply(punctuation_stopwords_removal)\n    \n    word_list = []\n    \n    for i, j in df_category.iterrows():\n        for word in j['combined']:\n            word_list.append(word)\n        \n    count_dict = Counter(word_list)\n    most_common_words_df = pd.DataFrame(count_dict.most_common(20), columns=['word', 'count'])\n    fig = px.histogram(most_common_words_df,\n                       x='word', \n                       y='count',\n                       title='Most common terms used while refering to a GitHub {}'.format(category),\n                       color_discrete_sequence=['#843B62'] )\n    fig.show()","metadata":{"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"df['combined'] = df['combined'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\ndf['combined'] = df['combined'].apply(lambda x: clean_text(x))\ndf.head()","metadata":{"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"df.drop(['title', 'body'], axis=1, inplace=True)\ndf.head()","metadata":{"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"\n\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)","metadata":{"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"#loading our BERT model\nBERT_UNCASED = '/kaggle/input/bert-base-uncased'","metadata":{"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"#loading the pre-trained BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(BERT_UNCASED)","metadata":{"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# some basic operations to understand how BERT converts a sentence into tokens and then into IDs\nsample_body = 'script stopped adding videos saenzramiro abc xyz'\ntokens = tokenizer.tokenize(sample_body)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(f' Sentence: {sample_body}')\nprint(f'   Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')","metadata":{"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"encodings = tokenizer.encode_plus(\n            sample_body,\n            max_length=32,\n            add_special_tokens=True,\n            return_token_type_ids=False,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n)\n\nencodings.keys()","metadata":{"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"print('Input IDs : {}'.format(encodings['input_ids'][0]))\nprint('\\nAttention Mask : {}'.format(encodings['attention_mask'][0]))","metadata":{"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 512","metadata":{"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"class GitHubCommitMessages(Dataset):\n    \n    def __init__(self, commit_message, label, tokenizer, max_len):\n        self.commit_message = commit_message\n        self.label = label\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.commit_message)\n    \n    def __getitem__(self, item):\n        commit_message = str(self.commit_message[item])\n        label = self.label[item]\n        \n        encoding = self.tokenizer.encode_plus(\n        commit_message,\n        add_special_tokens=True,\n        max_length=self.max_len,\n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        return_attention_mask=True,\n        return_tensors='pt')\n        return {\n        'commit_message': commit_message,\n         'input_ids': encoding['input_ids'],\n         'attention_mask': encoding['attention_mask'],\n         'label': torch.tensor(label, dtype=torch.long)\n          }","metadata":{"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"df = df[:2000]","metadata":{"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"training_data, testing_data = train_test_split(\n    df,\n    test_size=0.1,\n    random_state=RANDOM_SEED\n)\n\ntesting_data, validation_data = train_test_split(\n    testing_data,\n    test_size=0.5,\n    random_state=RANDOM_SEED\n)","metadata":{"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"training_data.shape, testing_data.shape, validation_data.shape","metadata":{"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"def create_data_loader(data, tokenizer, max_len, batch_size):\n    \n    ds = GitHubCommitMessages(commit_message=data.combined.to_numpy(),\n    label=data.label.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len)\n    \n    return DataLoader(\n    ds,\n    batch_size=batch_size,\n    num_workers=4)\n\n\nBATCH_SIZE = 16\ntrain_data_loader = create_data_loader(training_data, tokenizer, MAX_LENGTH, BATCH_SIZE)\ntesting_data_loader = create_data_loader(testing_data, tokenizer, MAX_LENGTH, BATCH_SIZE)\nval_data_loader = create_data_loader(validation_data, tokenizer, MAX_LENGTH, BATCH_SIZE)","metadata":{"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"df = next(iter(train_data_loader))\ndf.keys()","metadata":{"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"df['input_ids'].squeeze().shape, df['attention_mask'].squeeze().shape, df['label'].shape","metadata":{"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"print('commit_message  : ', df['commit_message'][0])\nprint('input_ids : ', df['input_ids'].squeeze()[0])\nprint('attention_mask : ', df['attention_mask'].squeeze()[0])\nprint('label : ', df['label'][0])","metadata":{"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"bert_model = BertModel.from_pretrained(BERT_UNCASED)","metadata":{"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"last_hidden_state, pooled_output = bert_model(\n  input_ids=encodings['input_ids'],\n  attention_mask=encodings['attention_mask']\n)","metadata":{"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"last_hidden_state.shape, pooled_output.shape","metadata":{"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"class BugPredictor(nn.Module):\n    \n    def __init__(self, n_classes):\n        super(BugPredictor, self).__init__()\n        self.bert_model = BertModel.from_pretrained(BERT_UNCASED)\n        self.dropout = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert_model.config.hidden_size, n_classes)\n        \n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert_model(\n        input_ids=input_ids,\n        attention_mask = attention_mask\n        )\n        output = self.dropout(pooled_output)\n        return self.out(output)\n        ","metadata":{"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"class_names = [0, 1, 2]\nbug_predictor_model = BugPredictor(len(class_names))\nbug_predictor_model = bug_predictor_model.to(device)","metadata":{"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 5\n\noptimizer = AdamW(bug_predictor_model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = total_steps\n)\n\nloss_fn = nn.CrossEntropyLoss().to(device)","metadata":{"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train()\n    \n    losses = []\n    correct_predictions = 0\n    \n    for d in data_loader:\n        input_ids = d['input_ids'].squeeze().to(device)\n        attention_mask = d['attention_mask'].squeeze().to(device)\n        targets = d['label'].to(device)\n\n        outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        \n        correct_predictions += torch.sum(preds==targets)\n        losses.append(loss.item())\n        \n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    \n    return correct_predictions.double()/n_examples, np.mean(losses)","metadata":{"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    \n    losses = []\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d['input_ids'].squeeze().to(device)\n            attention_mask = d['attention_mask'].squeeze().to(device)\n            targets = d['label'].to(device)\n\n            outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n\n            correct_predictions += torch.sum(preds==targets)\n            losses.append(loss.item())\n    \n    return correct_predictions.double()/n_examples, np.mean(losses)","metadata":{"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom collections import defaultdict\n\nhistory = defaultdict(list)\nbest_accuracy = 0\n\nfor epoch in range(EPOCHS):\n    print('EPOCH {}/{}'.format(epoch+1,EPOCHS))\n    print('-' * 10)\n    \n    train_acc, train_loss = train_model(bug_predictor_model, train_data_loader, loss_fn, optimizer, device, scheduler, len(training_data))\n    \n    print('Train loss : {} accuracy : {}'.format(train_loss, train_acc))\n    \n    val_acc, val_loss = eval_model(bug_predictor_model, val_data_loader, loss_fn, device, len(validation_data))\n    \n    print('Validation loss : {} accuracy : {}'.format(val_loss, val_acc))\n    \n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    \n    if val_acc > best_accuracy:\n        print('Saving the best model ...')\n        torch.save(bug_predictor_model.state_dict(), 'best_model.bin')\n        best_accuracy = val_acc\n    ","metadata":{"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"sample_bug_message = \" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque nisl eros \"","metadata":{"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"class_names = ['bug', 'feature', 'question']","metadata":{"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"def predict_git_category(sample_message, model):\n    encoded_message = tokenizer.encode_plus(sample_bug_message, max_length=MAX_LENGTH, add_special_tokens=True, return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')\n    input_ids = encoded_message['input_ids'].to(device)\n    attention_mask = encoded_message['attention_mask'].to(device)\n    \n    output = model(input_ids=input_ids, attention_mask=attention_mask)\n    _, prediction_idx = torch.max(output, dim=1)\n        \n    return class_names[prediction_idx]\n","metadata":{"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"print('Bug message : ', sample_bug_message)\nprint('Predicted Category for Github: ', predict_git_category(sample_bug_message, bug_predictor_model))","metadata":{"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}